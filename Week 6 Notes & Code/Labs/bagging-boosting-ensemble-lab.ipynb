{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Boosting Lab\n",
    "\n",
    "In this lab we will practice using Bagging and Boosting models on a dataset of your choice.\n",
    "\n",
    "---\n",
    "\n",
    "For this lab you should choose datasets in the past that have given you trouble with other models or that you are otherwise interested in looking at with bagging and boosting. The goal is so you get an idea for how they perform compared to more conventional regressions and classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and inspect the data\n",
    "\n",
    "Load a dataset or datasets that you have been difficult to get results on in the past, either from labs or projects.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 Inspect the data and create and X, y  classification and for regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import (LinearRegression, LogisticRegression, \n",
    "                                  Lasso, Ridge,\n",
    "                                  SGDRegressor, SGDClassifier)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "service_reviews = pd.read_csv('/home/llevin/Desktop/DSI-SF-2-llevin16/Week 6 Notes & Code/Datasets/service_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1362, 160) (1362,)\n"
     ]
    }
   ],
   "source": [
    "#Same process for service data\n",
    "y = service_reviews['target'].values\n",
    "X = service_reviews[[col for col in service_reviews.columns if col not in\\\n",
    "                  ['target','Class','stars','likes','business_id','Reviews']]]\n",
    "\n",
    "scale = StandardScaler()\n",
    "Xn = scale.fit_transform(X)\n",
    "\n",
    "print Xn.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.        ,  3.        ,  4.31818182, ...,  3.4       ,\n",
       "        3.80487805,  4.35      ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llevin/anaconda2/envs/dsi/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:466: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations\n",
      "  ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    1.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:    1.9s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:    3.5s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:    5.4s\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    6.4s finished\n"
     ]
    }
   ],
   "source": [
    "# Setup search parameters\n",
    "search_parameters = {\n",
    "    \"alpha\": np.linspace(0.0001, 1.0, 200)\n",
    "}\n",
    "\n",
    "lasso = Lasso()\n",
    "\n",
    "estimator = GridSearchCV(lasso, search_parameters, cv=5, verbose=1)\n",
    "estimator = estimator.fit(Xn,y)\n",
    "# Fit some data!\n",
    "best_lasso = estimator.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10748432438327793, 0.087043499801870539, 0.12398975236066567, 0.10057690932885133, 0.11841059674652155]\n",
      "Lasso:  0.107501016524\n",
      "Baseline:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llevin/anaconda2/envs/dsi/lib/python2.7/site-packages/sklearn/cross_validation.py:516: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=5.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    }
   ],
   "source": [
    "cv_indices = StratifiedKFold(y, n_folds=5,shuffle=True)\n",
    "\n",
    "lasso_scores = []\n",
    "\n",
    "for train,test in cv_indices:\n",
    "    x_train = Xn[train,:]\n",
    "    x_test = Xn[test,:]\n",
    "    y_train = y[train]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    best_lasso.fit(x_train,y_train)\n",
    "    \n",
    "    lasso_scores.append(best_lasso.score(x_test,y_test))\n",
    "\n",
    "print lasso_scores\n",
    "print 'Lasso: ',np.mean(lasso_scores)\n",
    "\n",
    "baseline_X = np.ones(Xn.shape[0])[:,np.newaxis]\n",
    "print 'Baseline: ',LinearRegression(fit_intercept=False).fit(baseline_X, y).score(baseline_X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Decision Tree Regressor\n",
    "\n",
    "1. Train a decision tree regressor on the regression problem\n",
    "- Evaluate the score with a 5-fold cross-validation\n",
    "- How does this compare to the model you fit on this data previously?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "## Define your DecisionTreeClassifier, search parameters, gridsearch\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "## Define your DecisionTreeClassifier\n",
    "dctc = DecisionTreeClassifier()\n",
    "\n",
    "## Search parameters\n",
    "dtc_params = {\n",
    "    'max_depth':[None,1,2,3,4],\n",
    "    'max_features':[None,'log2','sqrt','auto', 2,3,4,5],\n",
    "    'min_samples_split':[2,3,4,5,10,15,20,25,30,40,50]\n",
    "}\n",
    "\n",
    "## Gridsearch    \n",
    "dtc_gs = GridSearchCV(dctc, dtc_params, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3.0', '3.0', '4.3181', ..., '3.4', '3.8048', '4.35'], \n",
       "      dtype='|S6')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.asarray(y, dtype=\"|S6\")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 440 candidates, totalling 2200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    3.4s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:    4.9s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:    6.6s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:    7.9s\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:    9.9s\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:   12.8s\n",
      "[Parallel(n_jobs=1)]: Done 2200 out of 2200 | elapsed:   14.6s finished\n"
     ]
    }
   ],
   "source": [
    "best_dt = dtc_gs.fit(Xn, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_dt = best_dt.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.099315068493150679, 0.098976109215017066, 0.083636363636363634, 0.086956521739130432, 0.096385542168674704]\n",
      "DT:  0.0930539210505\n"
     ]
    }
   ],
   "source": [
    "cv_indices = StratifiedKFold(y, n_folds=5,shuffle=True)\n",
    "\n",
    "dt_scores = []\n",
    "\n",
    "for train,test in cv_indices:\n",
    "    x_train = Xn[train,:]\n",
    "    x_test = Xn[test,:]\n",
    "    y_train = y[train]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    best_dt.fit(x_train,y_train)\n",
    "    \n",
    "    dt_scores.append(best_dt.score(x_test,y_test))\n",
    "\n",
    "print dt_scores\n",
    "print 'DT: ',np.mean(dt_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Random Forest Regressor\n",
    "\n",
    "1. Train a random forest regressor on the regression problem and predict your dependent.\n",
    "- Evaluate the score with a 5-fold cross-validation\n",
    "- How does this compare to the model you fit on this data previously? How does it compare to the single decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define a Random Forest Classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "rf_params = {\n",
    "    'max_features':[None,'log2','sqrt', 2,3,4,5],\n",
    "    'max_depth':[1,2,3,None],\n",
    "    'min_samples_leaf':np.linspace(1,101,20),\n",
    "    'n_estimators':[100]\n",
    "}\n",
    "\n",
    "## gridsearch parameters, and cv =5\n",
    "rf_gs = GridSearchCV(rfc, rf_params, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 560 candidates, totalling 2800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   17.5s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:   56.1s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks       | elapsed:  1.8min\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks       | elapsed:  3.5min\n",
      "[Parallel(n_jobs=1)]: Done 1249 tasks       | elapsed:  5.1min\n",
      "[Parallel(n_jobs=1)]: Done 1799 tasks       | elapsed:  7.7min\n",
      "[Parallel(n_jobs=1)]: Done 2449 tasks       | elapsed: 12.1min\n",
      "[Parallel(n_jobs=1)]: Done 2800 out of 2800 | elapsed: 13.5min finished\n"
     ]
    }
   ],
   "source": [
    "best_rf = rf_gs.fit(Xn,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_rf = best_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.089403973509933773, 0.087248322147651006, 0.11152416356877323, 0.108, 0.13168724279835392]\n",
      "RF:  0.105572740405\n"
     ]
    }
   ],
   "source": [
    "cv_indices = StratifiedKFold(y, n_folds=5,shuffle=True)\n",
    "\n",
    "rf_scores = []\n",
    "\n",
    "for train,test in cv_indices:\n",
    "    x_train = Xn[train,:]\n",
    "    x_test = Xn[test,:]\n",
    "    y_train = y[train]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    best_rf.fit(x_train,y_train)\n",
    "    \n",
    "    rf_scores.append(best_rf.score(x_test,y_test))\n",
    "\n",
    "print rf_scores\n",
    "print 'RF: ',np.mean(rf_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Extra Trees Regressor\n",
    "\n",
    "1. Train an extra trees regressor on the regression problem and predict your dependent.\n",
    "- Evaluate the score with a 5-fold cross-validation\n",
    "- How does this model compare with the others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et = ExtraTreesClassifier()\n",
    "\n",
    "et.fit(Xn,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_et = et.fit(Xn,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.076666666666666661, 0.076388888888888895, 0.062271062271062272, 0.0728744939271255, 0.05905511811023622]\n",
      "ET:  0.0694512459728\n"
     ]
    }
   ],
   "source": [
    "cv_indices = StratifiedKFold(y, n_folds=5,shuffle=True)\n",
    "\n",
    "et_scores = []\n",
    "\n",
    "for train,test in cv_indices:\n",
    "    x_train = Xn[train,:]\n",
    "    x_test = Xn[test,:]\n",
    "    y_train = y[train]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    best_et.fit(x_train,y_train)\n",
    "    \n",
    "    et_scores.append(best_et.score(x_test,y_test))\n",
    "\n",
    "print et_scores\n",
    "print 'ET: ',np.mean(et_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. AdaBoost Classifier\n",
    "\n",
    "1. Train a AdaBoost classifier on your chosen classification problem.\n",
    "- Evaluate the classifier performance with a 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier()\n",
    "\n",
    "best_ada = ada.fit(Xn,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08191126279863481, 0.11583011583011583, 0.095890410958904104, 0.093283582089552244, 0.124]\n",
      "ADA:  0.102183074335\n"
     ]
    }
   ],
   "source": [
    "cv_indices = StratifiedKFold(y, n_folds=5,shuffle=True)\n",
    "\n",
    "ada_scores = []\n",
    "\n",
    "for train,test in cv_indices:\n",
    "    x_train = Xn[train,:]\n",
    "    x_test = Xn[test,:]\n",
    "    y_train = y[train]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    best_ada.fit(x_train,y_train)\n",
    "    \n",
    "    ada_scores.append(best_ada.score(x_test,y_test))\n",
    "\n",
    "print ada_scores\n",
    "print 'ADA: ',np.mean(ada_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. Gradient Boosted Trees Classifier\n",
    "\n",
    "\n",
    "1. Train a Gradient Boosting Trees classifier on your chosen classification problem.\n",
    "- Evaluate the score with a 5-fold cross-validation.\n",
    "- Compare with the AdaBoost score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1 477186735516371394580422328469528586785871701924042652699748206487659869458565672510032397330776041298855195703724998656.0000           12.72m\n",
      "         2 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000           10.49m\n",
      "         3 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000            9.63m\n",
      "         4 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000            9.15m\n",
      "         5 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000            8.84m\n",
      "         6 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000            8.60m\n",
      "         7 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000            8.40m\n",
      "         8 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000            8.24m\n",
      "         9 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000            8.09m\n",
      "        10 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000            7.96m\n",
      "        20 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000            6.88m\n",
      "        30 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000            5.96m\n",
      "        40 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000            5.13m\n",
      "        50 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000            4.26m\n",
      "        60 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000            3.40m\n",
      "        70 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000            2.55m\n",
      "        80 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000            1.70m\n",
      "        90 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000           51.48s\n",
      "       100 84842572795123807943517361051740657509094285276718651431138946819715415542502444237767395997842634179239436268859605771566577575912576650272909845730462492589502972616377252109731741029691950280163926736896.0000            0.00s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(verbose=1)\n",
    "\n",
    "best_gbc = gbc.fit(Xn,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1 113989998926572849741758178141384910843009408553002271617043885796992405544528711393954531351162200957507308666890486391830592455625768818309795154576702358757587354171893169801572827879249003796391819544741515697951239008453992637964178799639823853553729549755267113216453614204092416.0000            7.74m\n",
      "         2 150609738716817617085085294664412163386436191423256179712631436345959360429796092188244216615459726554228315125402870742170334531478960946935266334535704296601411226692238465541537063661181948068061758290748445702088422070438663302794247470751459761769801261008915822493898802114592768.0000            6.79m\n",
      "         3 150609738716817617085085294664412163386436191423256179712631436345959360429796092188244216615459726554228315125402870742170334531478960946935266334535704296601411226692238465541537063661181948068061758290748445702088422070438663302794247470751459761769801261008915822493898802114592768.0000            6.18m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-3f2eab240fc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mbest_gbc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mgbc_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_gbc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/llevin/anaconda2/envs/dsi/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[1;32m-> 1025\u001b[1;33m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/llevin/anaconda2/envs/dsi/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1078\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1079\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1080\u001b[1;33m                                      X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/llevin/anaconda2/envs/dsi/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m             residual = loss.negative_gradient(y, y_pred, k=k,\n\u001b[1;32m--> 760\u001b[1;33m                                               sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[1;31m# induce regression tree on residuals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/llevin/anaconda2/envs/dsi/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc\u001b[0m in \u001b[0;36mnegative_gradient\u001b[1;34m(self, y, pred, k, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m         \u001b[1;34m\"\"\"Compute negative gradient for the ``k``-th class. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m         return y - np.nan_to_num(np.exp(pred[:, k] -\n\u001b[1;32m--> 563\u001b[1;33m                                         logsumexp(pred, axis=1)))\n\u001b[0m\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n",
      "\u001b[1;32m/home/llevin/anaconda2/envs/dsi/lib/python2.7/site-packages/sklearn/utils/extmath.pyc\u001b[0m in \u001b[0;36mlogsumexp\u001b[1;34m(arr, axis)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;31m# the less errors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[0mvmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv_indices = StratifiedKFold(y, n_folds=5,shuffle=True)\n",
    "\n",
    "gbc_scores = []\n",
    " \n",
    "for train,test in cv_indices:\n",
    "    x_train = Xn[train,:]\n",
    "    x_test = Xn[test,:]\n",
    "    y_train = y[train]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    best_gbc.fit(x_train,y_train)\n",
    "    \n",
    "    gbc_scores.append(best_gbc.score(x_test,y_test))\n",
    "\n",
    "print gbc_scores\n",
    "print 'GBC: ',np.mean(gbc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. [BONUS] Use gridsearch to fine-tune models of your choice.\n",
    "\n",
    "1. What are the best parameters found with the gridsearch?\n",
    "2. How does the best score compare to the model(s) without cross-validation?\n",
    "\n",
    "**Be careful with many parameters! It can go slow.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [dsi]",
   "language": "python",
   "name": "Python [dsi]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
