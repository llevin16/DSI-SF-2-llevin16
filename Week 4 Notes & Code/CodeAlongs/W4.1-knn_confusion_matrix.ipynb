{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Confusion Matrix\n",
    "\n",
    "|   |Predicted Positive | Predicted Negative |   \n",
    "|---|---|---|\n",
    "|   |   |   |   \n",
    "|Actual Positive | TP = Correct!  | **FN**  |  \n",
    "|Actual Negative  | **FP**  | TN = Correct!  | \n",
    "\n",
    "- **True Positives**: A positive class observation (1) is correctly classified as positive by the model.\n",
    "- **False Positive**: A negative class observation (0) is incorrectly classified as positive.\n",
    "- **True Negative**: A negative class observation is correctly classified as negative.\n",
    "- **False Negative**: A positive class observation is incorrectly classified as negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load gridsearch\n",
    "from sklearn import grid_search\n",
    "\n",
    "\n",
    "# Load graph packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load the Spam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load the Spam Data set\n",
    "path_to_file = '/Users/Javier/Desktop/DSI-2-TEACH/DSI-SF/datasets/spam/spam_words_wide.csv'\n",
    "spam = pd.read_csv(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Find the Spam Baseline / Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## use value counts on is_spam\n",
    "print spam.is_spam.value_counts()\n",
    "print \"\\n\"\n",
    "\n",
    "## print the mean of is_spam\n",
    "print spam.is_spam.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Set up a KNN, with a test size of 0.3, a cv of 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define target = is_spam\n",
    "target = 'is_spam'\n",
    "\n",
    "# define predictors \"all the columns except target\" and called it cols\n",
    "cols = [c for c in spam.columns if c != target]\n",
    "\n",
    "# define x and y\n",
    "x = spam[cols]\n",
    "y = spam[target]\n",
    "\n",
    "# print y and x shapes\n",
    "print y.shape, x.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Train split the data, test_size = 0.3\n",
    "X_train, X_test, y_train, y_test =  train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "print y_train.shape, X_train.shape\n",
    "print y_test.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Intialize KNN \n",
    "## http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Setup our GridSearch Parmaters\n",
    "search_parameters = {\n",
    "    'n_neighbors':  range(1,10,2), \n",
    "    'weights':      (\"uniform\", \"distance\"),\n",
    "    'algorithm':    (\"ball_tree\", \"kd_tree\", \"brute\", \"auto\"),\n",
    "    'p':            [1,2]\n",
    "}\n",
    "\n",
    "# Intialize GridSearchCV\n",
    "clf = grid_search.GridSearchCV(knn, search_parameters, cv=5, verbose=1, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Fit data on train\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Best Estimator:\", clf.best_estimator_.n_neighbors\n",
    "print \"Best Params:\", clf.best_params_\n",
    "print \"Best Score:\", clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Predict the values call them y_pred (y predicted probabilities) and print 5 rows\n",
    "y_pred = clf.predict(X_test)\n",
    "print y_pred[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Build a Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Manually Calculate\n",
    "# Build a confusion Matrix\n",
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Confusion Matrix \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "confusion = pd.DataFrame(confmat, index=['is_not_spam_email', 'is_spam_email'],\n",
    "                         columns=['predicted_is_not_spam_email','predicted_is_spam_email'])\n",
    "\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Calculate true positives, false positives, true negatives, and false negatives from the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TP = confusion.ix['is_not_spam_email', 'predicted_is_not_spam_email']  \n",
    "\n",
    "FP = confusion.ix['is_spam_email', 'predicted_is_not_spam_email']\n",
    "\n",
    "TN = confusion.ix['is_spam_email', 'predicted_is_spam_email']\n",
    "\n",
    "FN = confusion.ix['is_not_spam_email', 'predicted_is_spam_email']\n",
    "\n",
    "print(zip(['True Positives','False Positives','True Negatives','False Negatives'],\n",
    "          [TP, FP, TN, FN]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Show that the accuracy is equivalent to: True Positives + True Negatives / Total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print((TP + TN) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Misclassification Rate (Error Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ((FP + FN))/ float(len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pcs = precision_score(y_test, y_pred)\n",
    "print pcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rcs = recall_score(y_test, y_pred)\n",
    "print rcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Create a classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "cls_rep = classification_report(y_test, y_pred)\n",
    "print cls_rep "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is what the classification report is telling us\n",
    "Each of the columns indicate an important metric for evaluating classification model performance.\n",
    "\n",
    "|   |   |\n",
    "|---|---|\n",
    "|**precision** | Ability of the classifier to avoid labeling a class as a member of another class. <br><br> `Precision = True Positives / (True Positives + False Positives)`<br><br>_A precision score of 1 indicates that the classifier never mistakenly classified the current class as another class.  precision score of 0 would mean that the classifier misclassified every instance of the current class_ |\n",
    "|**recall**    | is the ability of the classifier to correctly identify the current class. <br><br>`Recall = True Positives / (True Positives + False Negatives)`<br><br>A recall of 1 indicates that the classifier correctly predicted all observations.  0 means the classifier predicted all observations of the current class incorrectly.|\n",
    "|**f1-score** | is the harmonic mean of the precision and recall. The harmonic mean is used here rather than the more conventional arithmetic mean because the harmonic mean is more appropriate for averaging rates. <br><br>`F1-Score = 2 * (Precision * Recall) / (Precision + Recall)` <br><br>_The f1-score's best value is 1 and worst value is 0, like the precision and recall scores. It is a useful metric for taking into account both measures at once._ |\n",
    "|**support** | is simply the number of observations of the labelled class.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## David\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', target_names=False, figsize=(5, 5), cmap=plt.cm.Blues):\n",
    "    figure(figsize=figsize)\n",
    "    # confmat == the confusion_matrix result object (is an array)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    if target_names:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Precision\n",
    "# 0 vs. 1\n",
    "print(float(TP) / (TP + FP))\n",
    "\n",
    "# 1 vs. 0\n",
    "print(float(TN) / (TN + FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## How many class predictions did we \"recall\" correctly?\n",
    "# 0 vs. 1\n",
    "print(float(TP) / (TP + FN))\n",
    "\n",
    "# 1 vs. 0\n",
    "print(float(TN) / (TN + FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1-score is equivalent to: 2 * ( (Precision Recall) / (Precision + Recall) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0 vs. 1\n",
    "pos_precision = float(TP) / (TP + FP)\n",
    "pos_recall = float(TP) / (TP + FN)\n",
    "print(2. * (pos_precision * pos_recall) / (pos_precision + pos_recall))\n",
    "\n",
    "# 1 vs. 0\n",
    "neg_precision = float(TN) / (TN + FN)\n",
    "neg_recall = float(TN) / (TN + FP)\n",
    "print(2. * (neg_precision * neg_recall) / (neg_precision + neg_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remeber the Wisconsin Breast Cancer Dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0 means benign, 1 means malignant\n",
    "target    = [0, 1, 1, 0, 1, 0, 1, 1, 1]\n",
    "\n",
    "#Prediction Results from a Machine Leaning Model\n",
    "predicted = [0, 0, 1, 1, 1, 0, 1, 1, 1]\n",
    "\n",
    "# Label the Classifiers\n",
    "target_names = ['benign', 'malignant']\n",
    "\n",
    "#Print Classification Report\n",
    "print (classification_report(target, predicted, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(x=10):\n",
    "\n",
    "    target = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    predicted = ([1] * x) + ([0] * (30-x))\n",
    "    \n",
    "    print(\"Target   \", target)\n",
    "    print(\"Predicted\", predicted)\n",
    "    \n",
    "    # Label the Classifiers\n",
    "    target_names = ['benign', 'malignant']\n",
    "\n",
    "    #Print Classification Report\n",
    "    print(classification_report(target, predicted, target_names=target_names))\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interact(test, x=widgets.IntSlider(min=1, max=20, value=10));"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
